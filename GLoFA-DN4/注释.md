在colab中运行时，先拷贝colabEdit/GLoFA到sample_data

再从google drive下载mini_imagenet.tar



程序流程

~~~python
===== task arguments =====
data_name = mini_imagenet
network_name = resnet
model_name = glofa
N = 5
K = 1
Q = 15
===== experiment environment arguments =====
devices = [0]
flag_debug = False
n_workers = 8
===== optimizer arguments =====
lr_network = 0.000100
lr = 0.010000
point = (20, 30, 40)
gamma = 0.200000
wd = 0.000500
mo = 0.900000
===== training procedure arguments =====
n_training_episodes = 10000
n_validating_episodes = 200
n_testing_episodes = 10000
episode_gap = 200
===== model arguments =====
tau = 1.000000
delta = 1.000000
~~~

dataloader.generate_data_loader(data_path, flag_mode, n_episodes, N, S)

~~~python
# Train.py
一个episode中取80张图，80个标签
logits=model.forward(images) # glofa.forward()
~~~

glofa.py
~~~python
# glofa.py 
N=5,K=1,Q=15# 
resnet.encoder()
support_embedding # 5
query_embedding #75
set_function(640,640,level='task')

~~~



mask_task = self.f_task(support_embeddings, level='task').unsqueeze(0)

mask_class = self.f_class(support_embeddings, level='class').unsqueeze(0)



set_function.py就是MLP，可以从原文的公式11看出来

~~~python
# set_function level='task'
(psi): Sequential(
     (0): Linear(in_features=640, out_features=1280, bias=True)
     (1): ReLU()
     (2): Linear(in_features=1280, out_features=1280, bias=True)
     (3): ReLU()
     )
~~~

 $\phi(x)$
psi_output = self.psi(support_embeddings) 

 $[MLP(\phi(x));\phi(x)]$
rho_input = torch.cat([psi_output, support_embeddings], dim=1)

 $(\sum \limits_{x \in A}[MLP(\phi(x));\phi(x)])$
rho_input = torch.sum(rho_input, dim=0, keepdim=True)

$MLP(\sum \limits_{x \in A}[MLP(\phi(x));\phi(x)])$
rho_output = torch.nn.functional.relu6(self.rho(rho_input)) / 6 * self.args.delta 

---

**How to apply class-level masks to query instances?**

对于查询集中的图片$x_j$ , 每个类别都实验一下：用各自类别的$m^{cls}_n$处理支持集和查询集的embedding，选择距离最小的一个。$d_n=dis(\phi(x_j)\odot(1+m^{cls}_n),e_n)$ , $\odot$ 是元素对应相乘，$\lbrace e_n \rbrace_{i=n}^N$是mask之后的类中心，$e_n$对应 glofa.forward中的prototypes

~~~ 
prototypes_unnorm = torch.mean(masked_support_embeddings.view(self.args.K, self.args.N, -1), dim=0) # torch.Size([5, 640]) 
~~~

query_masked与prototypes相乘之和，乘积作为后验概率。

glofa.forward代码

~~~python
# N-5,K-5,Q-10
embeddings = self.encoder(images)  #torch.Size([75,640])
embeddings = embeddings.view(self.args.N * (self.args.K + self.args.Q), -1)  # [75,640]

support_embeddings = embeddings[:self.args.N * self.args.K, :] # [25,640]
query_embeddings = embeddings[self.args.N * self.args.K:, :] # [50,640]


mask_task = self.f_task(support_embeddings, level='task').unsqueeze(0) # ([1, 1, 640])
mask_class = self.f_class(support_embeddings, level='class').unsqueeze(0) # [1, 5, 640]

alpha = self.h(support_embeddings, level='balance').squeeze(0) # torch.Size([2]
[alpha_task, alpha_class] = alpha

masked_support_embeddings = support_embeddings.view(self.args.K, self.args.N, -1) * \
(1 + mask_task * alpha_task) * (1 + mask_class * alpha_class) # torch.Size([5, 5, 640])
prototypes_unnorm = torch.mean(masked_support_embeddings.view(self.args.K, self.args.N, -1), dim=0) # torch.Size([5, 640]) 
prototypes = F.normalize(prototypes_unnorm, dim=1, p=2) # torch.Size([5, 640])

masked_query_embeddings = query_embeddings.unsqueeze(0).expand(self.args.N, -1, -1) * \
(1 + mask_task * alpha_task) * (1 + mask_class.transpose(0, 1) * alpha_class) # torch.Size([5, 50, 640])
~~~

~~~python
mask_class # [1,50,640]

query_embeddings  # [50,640],5*10张图

unmasked_query_embeddings # torch.Size([5, 50, 640]) 把50张query复制5份，都乘上两个mask

masked_query_embeddings=query_embeddings *  mask_class# [5, 50, 640] = [5, 50, 640]*[5,1,640] 元素对应相乘
# 怎么理解两个三维矩阵相乘，masked_query_embeddings[i,:,:]=query_embeddings[i,:,:]*mask[i,:,:]，乘出来的二维矩阵堆放在masked_query_embeddings[i]的位置
# query_embeddings[5, 50, 640],相当于把query_embedding复制五份
# mask_class[5,1,640],来自self.f_class() 怎么理解？


prototypes # [5, 640] 5张support作为原型

prototypes.t().unsqueeze(0).expand(self.args.N, -1, -1) # [5, 640, 5] 5个原型都复制5份

logits = torch.bmm(masked_query_embeddings, prototypes.t().unsqueeze(0).expand(self.args.N, -1, -1))
# ([5, 50, 5]) 原型与query相乘，[a,b,c]内是某个query*某个local mask*某个prototype的相似度

x # 0~4

collapsed_logits = logits[x, :, x].t()  # ([50, 5])
~~~



~~~python
prototypes.t().unsqueeze(0).expand(self.args.N, -1, -1))
# .t().unsqueeze().expand()的过程是[5,640]->[640,5]->[1, 640, 5]->[5, 640, 5]
torch.bmm(masked_query_embeddings
~~~





~~~python
# 5-way 5-shot 10-query
mask_task # 一个
mask_class # 5个 [1, 5, 640]
masked_support_embeddings # [5, 5, 640] 5个原型,每个原型内有5个样本
prototypes # [5, 640] 类内平均后的5个原型
masked_query_embeddings # [5, 50, 640] 将50个query复制五份，与mask_class逐元素相乘，
# 计算masked_query_embeddings时， query的[i,j,k]元素是样本j的特征k，i=0~4是相同的， mask_class的[i,j,k]是类别i的特征k的平均mask， [i,Q样本,特征]*[S类别,j,特征mask]=[S类别,Q样本,特征*特征mask]

logits # [5,50,5] ,logits[i,j,k]代表[S类别的mask,Q样本,与S类别的相似度]
logits[x, :, x] # [5,50] 每次取出50个数，堆放成一行，重复5次，共5行，这50个数是[i类mask,Q样本,与i类的相似度]
collapsed_logits # [50, 5] ？[Q样本,用了S类别的mask与对应S类别]的相似度
~~~



